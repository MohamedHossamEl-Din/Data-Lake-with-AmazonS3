{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T15:36:25.739292Z",
     "iopub.status.busy": "2023-04-14T15:36:25.738824Z",
     "iopub.status.idle": "2023-04-14T15:36:57.670976Z",
     "shell.execute_reply": "2023-04-14T15:36:57.669993Z",
     "shell.execute_reply.started": "2023-04-14T15:36:25.739235Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a273e730ff5a423fb4d85e62eca79006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1681485983810_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-39-16.us-west-2.compute.internal:20888/proxy/application_1681485983810_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-42-190.us-west-2.compute.internal:8042/node/containerlogs/container_1681485983810_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No need for the credentials since I configured the EMR cluster  to without access keys option. But feel free to create them if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T15:54:19.574488Z",
     "iopub.status.busy": "2023-04-14T15:54:19.574158Z",
     "iopub.status.idle": "2023-04-14T15:54:19.841355Z",
     "shell.execute_reply": "2023-04-14T15:54:19.840513Z",
     "shell.execute_reply.started": "2023-04-14T15:54:19.574458Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77b5c47255648d7bd5a48dba6042f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#config = configparser.ConfigParser()\n",
    "#config.read('dl.cfg')\n",
    "\n",
    "#os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']\n",
    "#os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T15:37:13.190958Z",
     "iopub.status.busy": "2023-04-14T15:37:13.190583Z",
     "iopub.status.idle": "2023-04-14T15:37:13.426475Z",
     "shell.execute_reply": "2023-04-14T15:37:13.425223Z",
     "shell.execute_reply.started": "2023-04-14T15:37:13.190926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0068aeed673c43bb92e88baee3e37cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Creates SparkSession object.\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T16:12:07.208242Z",
     "iopub.status.busy": "2023-04-14T16:12:07.207784Z",
     "iopub.status.idle": "2023-04-14T16:12:07.456175Z",
     "shell.execute_reply": "2023-04-14T16:12:07.455096Z",
     "shell.execute_reply.started": "2023-04-14T16:12:07.208197Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b4639ee93a45d7bf3c42c1308ab20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Processes song data specified in the input_data path and creates tables for songs and artists. \n",
    "    Writes songs table to parquet files partitioned by year and artist.\n",
    "    write artists table to parquet files.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession object\n",
    "        SparkSession object to read and process the data.\n",
    "    input_data : string\n",
    "        path to read the dataset files.\n",
    "    output_data : string\n",
    "        path to write the processed data files.\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + \"song-data/A/A/A/*.json\"\n",
    "    \n",
    "    song_schema = R([\n",
    "        Fld(\"num_songs\", Int()),\n",
    "        Fld(\"artist_id\", Str()),\n",
    "        Fld(\"artist_latitude\", Dbl()),\n",
    "        Fld(\"artist_longitude\", Dbl()),\n",
    "        Fld(\"artist_location\", Str()),\n",
    "        Fld(\"artist_name\", Str()),\n",
    "        Fld(\"song_id\", Str()),\n",
    "        Fld(\"title\", Str()),\n",
    "        Fld(\"duration\", Dbl()),\n",
    "        Fld(\"year\", Int())\n",
    "        ])\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data, schema=song_schema)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]).dropDuplicates(subset=[\"song_id\"])\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'songs_table'), partitionBy=['year',  'artist_id'])\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select([\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"]).dropDuplicates(subset=[\"artist_id\"])\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'artists_table'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Processes log data specified in the input_data path and creates tables for users, time, and songplays. \n",
    "    write users table to parquet files.\n",
    "    write time table to parquet files partitioned by year and month.\n",
    "    write songplays table to parquet files partitioned by year and month.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession object\n",
    "        SparkSession object to read and process the data.\n",
    "    input_data : string\n",
    "        path to read the dataset files.\n",
    "    output_data : string\n",
    "        path to write the processed data files.\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + \"log_data/2018/*/*.json\" \n",
    "\n",
    "    # read log data file\n",
    "    df_log = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df_log.filter(df_log.page == 'NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select([\"userID\", \"firstName\", \"lastName\", \"gender\", \"level\"]).dropDuplicates(subset=[\"userID\"])\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table = users_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'users_table'))\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: datetime.fromtimestamp(x/1000).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    df = df.withColumn(\"timestamp\", get_timestamp(df.ts))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "#    get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000).strftime('%Y-%m-%d'))\n",
    "#    df = df.withColumn(\"datetime\", get_datetime(df_log.ts))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.selectExpr(\n",
    "        \"timestamp as start_time\",\n",
    "        \"hour(timestamp) as hour\",\n",
    "        \"dayofmonth(timestamp) as day\",\n",
    "        \"weekofyear(timestamp) as week\",\n",
    "        \"month(timestamp) as month\",\n",
    "        \"year(timestamp) as year\",\n",
    "        \"dayofweek(timestamp) as weekday\"\n",
    "    )\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(output_data+ \"time_table\")\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.json(input_data + \"song-data/A/A/A/*.json\")\n",
    "    df = df.withColumn('songplay_id', F.monotonically_increasing_id())\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    df.createOrReplaceTempView(\"log_table\")\n",
    "    song_df.createOrReplaceTempView(\"song_table\")\n",
    "    \n",
    "    songplays_table = spark.sql(\"\"\"SELECT l.songplay_id as songplay_id,\n",
    "                                            l.timestamp as start_time,\n",
    "                                            l.userID as user_id,\n",
    "                                            l.level as level,\n",
    "                                            s.song_id as song_id,\n",
    "                                            s.artist_id as artist_id,\n",
    "                                            l.sessionId as session_id,\n",
    "                                            l.location as location,\n",
    "                                            l.userAgent as user_agent,\n",
    "                                            year(l.timestamp)as year,\n",
    "                                            month(l.timestamp) as month\n",
    "                                    FROM log_table as l\n",
    "                                    JOIN song_table as s\n",
    "                                    ON (l.artist = s.artist_name) AND (l.song = s.title) AND (l.length = s.duration)\n",
    "                                    WHERE l.userID IS NOT NULL\n",
    "                                        \"\"\")\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(output_data+ \"songplay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T15:37:17.411089Z",
     "iopub.status.busy": "2023-04-14T15:37:17.410757Z",
     "iopub.status.idle": "2023-04-14T15:37:18.432268Z",
     "shell.execute_reply": "2023-04-14T15:37:18.431108Z",
     "shell.execute_reply.started": "2023-04-14T15:37:17.411060Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cb8d06e2e6425790782dc98bd53182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://my-data-lake-bucket1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T16:12:11.275033Z",
     "iopub.status.busy": "2023-04-14T16:12:11.274653Z",
     "iopub.status.idle": "2023-04-14T16:13:09.575559Z",
     "shell.execute_reply": "2023-04-14T16:13:09.574567Z",
     "shell.execute_reply.started": "2023-04-14T16:12:11.275001Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108fb9bbcd7748959da7eef348eedae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_song_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T15:47:45.655048Z",
     "iopub.status.busy": "2023-04-14T15:47:45.654520Z",
     "iopub.status.idle": "2023-04-14T15:49:12.354074Z",
     "shell.execute_reply": "2023-04-14T15:49:12.353240Z",
     "shell.execute_reply.started": "2023-04-14T15:47:45.655015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19db78aa16f449f8280fdd8109a0ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_log_data(spark, input_data, output_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
